---
layout : post
title :  Recently personal view on the parallel training of neural networks
category : neural network
tags : [neural network, parallel training, GPGPU]
---
{% include JB/setup %}

Neural/perceptron networks are weights and biases terms (or called "Parameters") linked to input features using different structures, called "Topology". The optimization of networks (or called "Training") follows a goal as simple as the one used in logistic regression, to minimize a global "Cost" function. Here comes the fun part:

## Some facts

+ The cost function is calculated by comparing the network output and the desired output (or called "Label"). It works like a punishment. It tells you how far you are from your goal, the location where you get minimal cost.

+ Once the cost is known, it is time to try to move closer to that location. And how is that accomplished? By using "Gradient Descent".

>> Suppose a hiker is in the mountains, surrounded by trees and bushes. He knows there is a lowest spot somewhere in the valley and he wants to get there to join a party. But he cannot see straight so what he is going to do? He steps forward towards the downhill direction, determined by his current location. For how long should he move? He could move tens or thousands or meters and determine the downhill direction again from his new location. That's up to him.

>> This example is often used to mimic the training of neural networks. The cost function forms a manifold of the parameters. When different values of the parameters are chosen, the hiker is place at different locations in the mountains. The lowest spot of the valley means the set of certain parameters which gives the lowest cost. The downhill direction is the negative "Gradient" of cost function, while the gradient is the derivative of cost function w.r.t. the parameters which defines the direction of increase, i.e. the uphill direction. By updating the parameters in "Gradient Descent" direction, an "Epoch" of training is finished. The length of this update is the "Learning Rate". The repeat of this process is called the "Iteration" of GD algorithm.

>>> Now comes the tricky part. Unfortunately, our cost manifold is often not a plain valley, it is rather a mire, filled with faked partying spots by wicked witches, which are called "Local Optima", compared with where the real party is going on, i.e. the "Global Optima". How is it so? There are two reasons. One is that the number of parameters is huge. For ASR tasks there are usually millions of or even nearly a billion of parameters. The other one dues to the scale of data. The cost function accumulates the cost of multiple samples(or called "Training Data" for which we have both input "Feature" and "Label"). In the mountain climbing story, the hiker stands on a solid ground because all the samples in the training data are used to calculate cost. The GD process running in this flavor is called "Batch Gradient Descent". In practical scenarios, this is impractical because it creates many "Local Optima" on the cost manifold. Alternatively, we can shuffle the training data and use small fractions of the whole set to reduce the number of "Local Optima" so that "Gradient Descent" does not lead us to very poor "Local Optima". A fraction can be a fixed number of samples, or one sample at a time, namingly "Mini-batch Stochastic Gradient Descent" and "On-line Stochastic Gradient Descent" respectively. The process is, initialize a starter point, run a fraction, get to a new point, repeat. Simple. Now a "Epoch" means all the samples are dealt with exactly once. Rumor says that MSGD/OSGD is for accelerating the training. But actually, it was for deforming the cost manifold, sort of a "Regularization". The acceleration is just a cheerful side-effect.

>> Now that we know how to calculate cost, and how to update parameters using gradients in different flavors, how to calculate gradients for all the parameters of the network? The algorithm used here is called "Error Back Propagation" introduced by Geoff Hinton. It helps you determine quantitatively how much each parameter contribute to the final cost of the batch/sample. This contribution is used to calculate the "Gradient" w.r.t. that parameter. The derivation of EBP depends on the "Topology" of the network. It can be easily implemented by the chain rule of derivatives.

+ Now that we know how to calculate cost, how to calculate the gradients and how to update the parameters. It's time to think about parallelization.

## Parallel training

It's not fun? OK, the block above aims to introduce the basic idea. Now consider the means of parallelizing the process. The reason of doing this is that the "Training" takes a long time when we train very large networks with a huge amount of data. Even if the training algorithms are implemented using GPGPU devices with CUDA support, the training of acoustic model using thousands of hours of speech can take days or weeks. Now think of the elements that can be defined as the parallel atoms. Which variable above are counted as thousands or millions? There are two of them, the number of data and the number of parameters. They lead to two different paradigms, the "Data Parallelization" and the "Model Parallelization".

+ Let's talk about MP first. Why? Because there are not many to talk about. MP generally requires deliberately tuning of model structure, so that different parts of the network can be trained independently on different computing nodes. One attempt is called the Pipelined EBP algorithm. Different neurons/units deal with different batches of samples, in a parallel way. Another attempt is to deal with one batch using several computing nodes. Different nodes stores different part of the network and communicate with each other when necessary. Multi-DNN is another example of MP. MP methods might be workable, but usually very tricky, thus not very generalizable.

+ DP is much better developed then MP. The idea is to let the computing nodes process different samples independently. The ultimate goal is to achieve the same performance as single-node while accelerate the training approximately linearly to the number of nodes.

>> Suppose we have K nodes. A straightforward way is to distribute a batch of samples cross the computing nodes. Each nodes process a 1/K fraction of the samples and output a sub-gradient. A master collect all the sub-gradients, accumulate them to get the gradient of this batch, update the network, and distribute the updated network cross the computing nodes. Such method is called "Synchronous Stochastic Gradient Descent" because all the nodes needs to wait for the slowest one to finish its job. This paradigm worked when the computation were done using CPUs. In CPU, the gradient computation of 1024 samples is 4 times faster than that of 4096 samples. When the computations are done using GPGPU devices, it does not speed up the training.

>>> Alternatively, SGD can be ran asynchronously. At the beginning, the master distribute the initial network cross the computing nodes. Each node stores a 1/K fraction of the training data and works on its own. After the local network is updated using certain number of batches, the node broadcast its newest network to all other nodes. The master keeps track of the newest version of the network. This kind of methods are called "Asynchronous Stochastic Gradient Descent". A tricky issue is the broadcasting frequency, i.e. the number of batches processed between two broadcasts. If it is high, the nodes would be busy receiving the networks. If it is low, the nodes would be receiving stale parameters.This algorithm has another interesting name "Hogwild!". It's like a hog running at randomly directions since the training samples are distributed cross the nodes randomly. If the directions are stale, the hog went wild and the network corrupts. However, due to the sparsity of the training data of large scale tasks, the sub-gradients can be thought as orthogonal. Thus "Hogwild!" works for most of the time. Then comes the GPGPU framework. While the problem of computational speed is solved, the transmission speed of the gradient/parameters becomes the bottleneck. Recent attempts focus on two opposite possibilities, infinite band router and heavy compression of gradients. In 2013, MSR proposed the equations to estimate the efficiency of parallelization (ICASSP2014), i.e. the optimal number of nodes given computing power and task scale. In 2014, they proposed a 1-bit ASGD framework (IS2014), which quantifies each gradient using 1 bit (larger then 0 or not) and add the quantization error into the respective next minibatch gradient before quantization. This largely saves the communication time, but was only tested under SSGD framework. Moreover, a group at CMU (IS2014) tried some simple techniques. They use momentum on master node and weighted the collected gradients according to their "staleness". Regardless of what's going on outside, Google stick to their DistBelief framework to parallelize all kinds or networks, and applied sequence training (ICASSP2014).

+ Since DP is still the dominating approach, and there is no rising star, I'll still focus on it. While different network topologies require variances of EBP to get the gradients of one fraction of samples, DP tries to use these distributed orthogonal gradients in a parallel and efficient way. The research on these two topics can benefit from each other.

+ There are other more theoretical attempts, such as regularizing the cost manifold (by Baidu IDL, KDD2014), second-order method for more stable gradient (by JHU, AISTATS2015), etc.

In the end, as Yoshua Bengio recently said on Quora.com, parallel neural network training on a cluster of GPU nodes is still challenging.

## Conclusion
There is a lot of work to do.